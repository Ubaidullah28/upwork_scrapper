Search Query,Title,Job Link,Tags,Client Spent,Payment Info,Budget Type,Lower Hourly Rate,Higher Hourly Rate,Fixed Price,Payment Verified/Unverified,Description,Posted Time
Data Engineering,ETL Specialist Needed for Transforming Supplier CSV Data,https://www.upwork.com/jobs/ETL-Specialist-Needed-for-Transforming-Supplier-CSV-span-class-highlight-Data-span_~021953444504694934894/?referrer_url_path=/nx/search/jobs/,"Data Entry, Python, ETL Pipeline","$10K+
spent","Est. time:
1 to 3 months, Less than 30 hrs/week",Hourly: Intermediate,$8.00,$25.00,N/A,Payment verified,"We are looking for an experienced ETL specialist to help us automate and standardize the transformation of product data from our suppliers. We receive product data from four different suppliers, each with their own formatting, twice a year. Currently, importing this into our webshop is a time-consuming and error-prone process due to inconsistencies in structure and content. Your task: Develop an ETL solution that can automatically transform and standardize this supplier into our universal master format, ready for import into our webshop. Ideally, the output will be used in Airtable or a similar platform, functioning as a lightweight PIM (Product Information Management) system. Requirements: - Proven experience with ETL tools or custom-built pipelines - Strong skills in processing and transforming structured (especially CSV) - Ability to design reusable and scalable workflows for recurring imports - Familiarity with platforms like Airtable, SeaTable, Baserow, or similar tools - Basic understanding of e-commerce product (e.g. SKUs, sizes, variants, etc.) - Attention to detail and a focus on quality Bonus points for: - Experience with AI-assisted mapping (e.g. recognizing dominant color, material types) - Integration knowledge with Shopify or Magento - Experience building mini PIMs or internal management systems What we provide: - Examples of supplier files - Documentation of our preferred structure and field definitions - Support during implementation and testing If you’re passionate about building efficient, no-nonsense workflows and enjoy cleaning up messy , we’d love to hear from you!",2025-08-07 18:15:00
Data Engineering,Senior Data Engineer,https://www.upwork.com/jobs/Senior-span-class-highlight-Data-span-Engineer_~021953412962093392361/?referrer_url_path=/nx/search/jobs/,"Data Engineering, Data Management, Data Science, Data Modeling","$1M+
spent","Est. time:
3 to 6 months, 30+ hrs/week",Hourly: Expert,$45.00,$75.00,N/A,Payment verified,"We are looking for a senior data engineer who can thrive in an environment where initiative and self-management is emphasized. You'd be working in a Python codebase, managing the data pipeline which loads from third party sources into versioned postgres databases, using dbt and custom Python code. Developers who also have some Building Science experience will be prioritized in the interview process (though, this is not a requirement). The client is in the climatetech space, and the success of the product depends upon being able to accurately describe and model buildiings. Requirements: * 10+ years of software experience (demonstrated on your resume) with an emphasis on * Experience building & running a pipeline at scale * Experience with DBT * Significant experience and expertise in Python * Significant experience and expertise with SQL * Understanding of OLAP oriented schema design * Demonstrable experience optimizing database schemas * Experience with postgres, with a good understanding of the internals and optimization Interview process: * Async video code interview - 1 hour * 1-2 rounds of technical interviews - 2-3 hours To have your application considered, please ensure you meet these criteria in your application: * You have attached a PDF resume (yes, Upwork _does_ allow this) which includes your education and details of experience since * You have written a no-BS zero-GPT cover note which briefly states why you are specifically relevant to this position (don't worry about writing too much here; we're primarily looking at experience level, though we do want to understand why you believe this to be a good fit) * You actually do have 10+ years of relevant experience If you're a good fit, this is a highly rewarding work environment, with significant autonomy, and opportunity to make a significant difference.",2025-08-07 16:50:02
Data Engineering,Senior Data Engineer – Snowflake | ADF | Python | APIs | Snowflake | Airflow | U.S. time zone,https://www.upwork.com/jobs/Senior-span-class-highlight-Data-span-Engineer-Snowflake-ADF-Python-APIs-Snowflake-Airflow-time-zone_~021953330444646413085/?referrer_url_path=/nx/search/jobs/,"Data Transformation, Data Engineering, Data Warehousing & ETL Software, Snowflake","$1K+
spent","Est. time:
More than 6 months, 30+ hrs/week",Hourly: Expert,$10.00,$15.00,N/A,Payment verified,"We are seeking a highly skilled Data Engineer to help build and scale our data pipelines within Snowflake. The initial phase will focus on ingesting data from Salesforce, with future expansion to other systems. You’ll be working with modern ELT tools and orchestrators, and will play a critical role in integrating AI agent workflows through Python-based API development. Responsibilities: • Ingest into Snowflake from multiple sources, starting with Salesforce. • Design and manage ELT pipelines using tools such as Airbyte, Airflow, or Azure Factory (ADF) (ADF preferred). • Write advanced SQL for modeling and transformation within Snowflake. • Develop and maintain Python APIs, especially for AI agent integration. • Ensure high performance, reliability, and quality across the stack. • Collaborate with cross-functional teams to deliver -driven solutions. Requirements: • 8+ years of experience in or related roles. • Strong expertise with Snowflake (required). • Advanced skills in SQL (preferred) and Python (required). • Experience with API development and external service integration. • Hands-on experience with Airbyte, Airflow, or ADF (ADF highly preferred). • Bachelor’s degree in Computer Science or a related field. • Must be available to work during U.S. time zones (full 8 hours in EST or PST).",2025-08-07 10:50:05
Data Engineering,AI Interview Tool Tester,https://www.upwork.com/jobs/Interview-Tool-Tester_~021953297408397608631/?referrer_url_path=/nx/search/jobs/,"Data Engineering, Python, Artificial Intelligence, Software Testing","$0
spent","Est. time:
Less than 1 month, Less than 30 hrs/week",Hourly: Intermediate,$15.00,$20.00,N/A,Payment verified,"Job Description: AI Interview Tool Tester (Data Engineer) We're looking for a skilled Data Engineer to test our new AI-powered interview tool. This is a freelance, project-based role where you will participate in mock interviews to evaluate and provide feedback on the tool's performance. You will be compensated at a rate of $15-$20 per interview. Responsibilities 💻 Participate in AI-driven interviews as a mock candidate. Evaluate the tool's performance by testing its ability to accurately assess your knowledge and skills. Provide detailed, constructive feedback on the interview experience, including question relevance, technical accuracy, and the tool's overall user-friendliness. Identify potential biases or inaccuracies in the AI's responses and assessments. Document and report bugs or performance issues clearly and concisely. Requirements 🛠️ Proven experience as a Engineer, with a strong understanding of pipelines, ETL processes, and big technologies. Expertise in SQL and Python is essential. Familiarity with cloud platforms (e.g., AWS, GCP, Azure) and warehousing solutions. Excellent written and verbal communication skills to provide clear and detailed feedback. Ability to think critically and analytically to assess the AI's performance.",2025-08-07 08:50:08
Data Engineering,"Freelance Data Engineer – GCP Pipelines, Data Integration & AI Tagging",https://www.upwork.com/jobs/Freelance-span-class-highlight-Data-span-Engineer-GCP-Pipelines-span-class-highlight-Data-span-Integration-amp-Tagging_~021953412454509376285/?referrer_url_path=/nx/search/jobs/,"LLM Prompt Engineering, ETL Pipeline, Vertex AI, API Integration, Google Cloud Platform","$100+
spent","Est. time:
1 to 3 months, 30+ hrs/week",Hourly: Intermediate,$18.00,$38.00,N/A,Payment verified,"We’re building a tool for impact investors and need a freelance data engineer to: - Integrate Bright Data + user DB into BigQuery - Build Vertex AI Pipelines to orchestrate ingestion, cleaning, and classification - Improve data quality (deduplication, validation, harmonization) - Create a prompt-based GPT tagging tool for non-tech users - Ensure full GDPR compliance across workflows - Deliver clear pipeline docs & update playbook",2025-08-07 16:50:11
Data Engineering,End-to-End Data Platform Architect Needed,https://www.upwork.com/jobs/End-End-span-class-highlight-Data-span-Platform-Architect-Needed_~021953327449143707319/?referrer_url_path=/nx/search/jobs/,"JavaScript, Python, Web Development, HTML, CSS","$50K+
spent","Est. time:
1 to 3 months, Less than 30 hrs/week",Hourly: Expert,$50.00,$75.00,N/A,Payment verified,"We are seeking an experienced architect to design and implement an end-to-end data platform utilizing an open-source framework with enterprise capabilities. The ideal candidate will have a strong background in data architecture and experience with various technologies. You will work closely with our team to ensure that the platform meets our business needs, scalability, and security requirements. If you have a proven track record of building robust solutions, we would love to hear from you!",2025-08-07 10:50:13
Data Engineering,Data Pipeline Modification Expert Needed,https://www.upwork.com/jobs/span-class-highlight-Data-span-Pipeline-Modification-Expert-Needed_~021953338352018851511/?referrer_url_path=/nx/search/jobs/,"Data Scraping, Data Mining, Python, ETL Pipeline, Microsoft Excel","$93
spent","Est. time:
1 to 3 months, Less than 30 hrs/week",Hourly: Intermediate,$8.00,$25.00,N/A,Payment verified,"We are seeking an experienced professional to make necessary changes to our existing data pipeline. The ideal candidate should have a strong background in data engineering and be able to identify areas of improvement to enhance efficiency and reliability. You'll be responsible for analyzing current workflows, implementing changes, and ensuring seamless flow. Excellent problem-solving skills and attention to detail are crucial for this role.",2025-08-07 11:50:15
Data Engineering,SQL MESH Developer Needed for Data Integration Project,https://www.upwork.com/jobs/SQL-MESH-Developer-Needed-for-span-class-highlight-Data-span-Integration-Project_~021953396553209597294/?referrer_url_path=/nx/search/jobs/,"Python, SQL, Microsoft SQL Server, MySQL, C#","$0
spent","Est. time:
1 to 3 months, 30+ hrs/week",Hourly: Intermediate,$7.00,$8.00,N/A,Payment verified,"Job Summary: We are seeking an experienced SQL Mesh Engineer to develop and manage complex SQL workflows, ensuring seamless collaboration and governance across our data engineering teams. The ideal candidate will be responsible for building and maintaining pipelines, managing version control and lineage, and ensuring efficient, scalable workflows using SQL Mesh. Key Responsibilities: Pipeline Management: Develop, maintain, and optimize SQL pipelines using SQL Mesh, ensuring smooth flow across the organization. Version Control & Lineage: Implement and manage version control for SQL scripts, track lineage, and ensure that models are well-documented. Collaboration & Governance: Work closely with teams to implement governance best practices, ensuring the consistency and quality of across pipelines. SQL Query Optimization: Optimize SQL queries for performance and scalability, ensuring timely and accurate processing. Automation & Scheduling: Automate recurring tasks, including job scheduling, testing, and reporting, to improve operational efficiency. Troubleshooting & Debugging: Identify and resolve issues within workflows, including quality issues and performance bottlenecks. Testing & Quality Assurance: Develop and implement testing strategies for SQL workflows to ensure integrity and reliability. Documentation: Document all SQL workflows, including their dependencies, transformations, and business logic, to facilitate team collaboration and knowledge sharing. Required Skills & Qualifications: Bachelor's degree in Computer Science, , or a related field. Proven experience working with SQL databases (MySQL, PostgreSQL, SQL Server, or similar RDBMS). Hands-on experience with SQL Mesh or similar workflow management platforms. Strong understanding of lineage, version control, and governance principles. Experience in SQL query optimization for high-performance processing. Familiarity with cloud platforms (AWS, GCP, Azure) and related services. Strong problem-solving skills and ability to troubleshoot and debug complex SQL workflows. Ability to work collaboratively in a team environment and communicate effectively with non-technical stakeholders.",2025-08-07 15:50:18
Data Engineering,Junior Data Engineer – Mapping & Dataset Management (Contract),https://www.upwork.com/jobs/Junior-span-class-highlight-Data-span-Engineer-Mapping-amp-Dataset-Management-Contract_~021953117441952866793/?referrer_url_path=/nx/search/jobs/,Python,"$100K+
spent","Est. time:
More than 6 months, 30+ hrs/week",Hourly Entry Level,N/A,N/A,N/A,Payment verified,"Junior Engineer – PDF Field Mapping & Dataset Management (Contract) Client: ThriveLink (telephonic AI agents for hospitals and health plans) Engagement: hourly on Upwork (about 10–15 hrs/week to start) Growth path: can expand to 20–40 hrs/week and convert to full-time as the company scales Why we’re hiring We need a meticulous junior engineer to map structured fields to specific PDF templates so we can generate accurate, ready-to-submit forms at scale. You will turn a dictionary into working PDF outputs by creating, testing, and maintaining field-to-PDF mappings. What you’ll do - Map canonical fields to PDF inputs (AcroForm fields or coordinate-based overlays) across multiple form templates. - Create and maintain mapping specs in JSON or YAML with clear field names, types, formatting rules, and fallbacks. - Build small scripts (Python) to fill, stamp, or overlay into PDFs and export flattened files. - Handle checkboxes, radio buttons, multi-line text, wrapped text, and conditional fields. - Manage versions of templates; track changes and deprecations in GitHub. - Run QA checks on generated PDFs (spot checks, pixel/coordinate sanity checks, and value validation). - Redact test when needed; follow HIPAA and PHI handling guidelines. Example deliverables (first 2–4 weeks) - Mapping files for 3–5 PDF templates with complete field coverage and documented assumptions. - A Python CLI to fill a template from a sample JSON payload and output a flattened PDF. - A validation checklist and sample outputs showing correct placement, wrapping, and checkbox/radio behavior. - A short README that explains how to run the filler, update mappings, and add new templates. Must-have skills: - Basic Python (pandas optional), command-line comfort, and JSON/YAML proficiency. - Experience with at least one PDF library (PyPDF2, pdfrw, reportlab, pikepdf, or similar). - Precision with coordinates, spacing, and typography basics (font size, line height, margins). - Git/GitHub for version control and pull requests. - Clear written communication and attention to detail. Nice-to-have skills - Adobe Acrobat form editing or similar tools for inspecting field names and positions. - pdfplumber or pdfminer for extracting coordinates and text. - Experience with government or healthcare forms, checkboxes, and conditional logic. - Basic understanding of PHI handling and HIPAA awareness training.",2025-08-07 18:50:20
Data Engineering,Data Engineer,https://www.upwork.com/jobs/span-class-highlight-Data-span-Engineer_~021953155039752643049/?referrer_url_path=/nx/search/jobs/,"Elasticsearch, Redis, MongoDB, Python, Node.js","$900K+
spent","Est. budget:
$600.00",Fixed price Intermediate,N/A,N/A,$600.00,Payment verified,"Full-Time Senior Data Engineer (10 AM – 5 PM) – Individual Applicants Only Description: We are seeking an experienced Senior Data Engineer to help us manage, update, and maintain a large-scale 10 TB data infrastructure. This is a full-time role (10 AM – 5 PM), and only individual freelancers (no agencies or teams) will be considered. Tech Stack: We work with technologies like: - ElasticSearch - Redis - MongoDB - Python and Node.js Responsibilities: - Maintain and optimize our 10 TB+ systems - Implement efficient cleaning and archiving strategies - Delete or archive outdated records to improve system performance - Ensure reliability and system scalability Requirements: - Senior-level experience (please do not apply if you're a junior or need constant guidance) - Strong problem-solving skills and a proactive mindset - Proven background working with large-scale environments - Ability to work independently and deliver results without hand-holding If you're a self-sufficient professional who knows how to get things done, we’d love to hear from you.",2025-08-07 18:50:23
